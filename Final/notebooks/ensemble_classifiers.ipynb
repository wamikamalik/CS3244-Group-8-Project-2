{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from random import randint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done renaming\n"
     ]
    }
   ],
   "source": [
    "# Standardising the naming convention across all folders (Not part of genetic algorithm)\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/ensemble_test_hand'\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    os.chdir(HANDS_DATA_PATH)\n",
    "    curr_dir_path = HANDS_DATA_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(curr_dir_path):\n",
    "        img_num, file_ext = os.path.splitext(filename)\n",
    "        new_img_num = str(int(img_num))\n",
    "\n",
    "        if (new_img_num != img_num):\n",
    "            old_filepath = os.path.join(curr_dir_path, img_num + file_ext)\n",
    "            new_filepath = os.path.join(curr_dir_path, new_img_num + file_ext)\n",
    "            os.renames(old_filepath, new_filepath)\n",
    "\n",
    "print(\"Done renaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering common images\n"
     ]
    }
   ],
   "source": [
    "# Extracting common images (Not part of genetic algorithm)\n",
    "LATEST_ORIG_DATA_PATH = '/Users/preshita/Desktop/latest_combined_orig/test'\n",
    "LATEST_FACE_DATA_PATH = '/Users/preshita/Desktop/latest_combined_face/test'\n",
    "LATEST_HAND_DATA_PATH = '/Users/preshita/Desktop/latest_combined_hand/test'\n",
    "\n",
    "ENSEMBLE_ORIG_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/orig/test'\n",
    "ENSEMBLE_FACE_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/face/test'\n",
    "ENSEMBLE_HAND_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/hand/test'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    latest_orig_dir_path = LATEST_ORIG_DATA_PATH + '/' + class_label\n",
    "    latest_face_dir_path = LATEST_FACE_DATA_PATH + '/' + class_label\n",
    "    latest_hand_dir_path = LATEST_HAND_DATA_PATH + '/' + class_label\n",
    "    \n",
    "    ensemble_orig_dir_path = ENSEMBLE_ORIG_DATA_PATH + '/' + class_label\n",
    "    ensemble_face_dir_path = ENSEMBLE_FACE_DATA_PATH + '/' + class_label\n",
    "    ensemble_hand_dir_path = ENSEMBLE_HAND_DATA_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(latest_face_dir_path):\n",
    "        if (os.path.exists(latest_orig_dir_path + '/' + filename) and \n",
    "            os.path.exists(latest_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(latest_orig_dir_path + '/' + filename, ensemble_orig_dir_path + '/' + filename)\n",
    "            shutil.copy(latest_face_dir_path + '/' + filename, ensemble_face_dir_path + '/' + filename)\n",
    "            shutil.copy(latest_hand_dir_path + '/' + filename, ensemble_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done filtering common images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying the relevant missing images\n"
     ]
    }
   ],
   "source": [
    "COMBINED_ORIG_TRAIN_PATH = '/Users/preshita/Desktop/combined_new/train'\n",
    "COMBINED_FACE_TRAIN_PATH = '/Users/preshita/Desktop/combined_new_face/new_train'\n",
    "COMBINED_HAND_TRAIN_PATH = '/Users/preshita/Desktop/combined_new_hand/new_train'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    combined_orig_dir_path = COMBINED_ORIG_TRAIN_PATH + '/' + class_label\n",
    "    combined_face_dir_path = COMBINED_FACE_TRAIN_PATH + '/' + class_label\n",
    "    combined_hand_dir_path = COMBINED_HAND_TRAIN_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(combined_orig_dir_path):\n",
    "        if (not os.path.exists(combined_face_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_face_dir_path + '/' + filename)\n",
    "            \n",
    "        if (not os.path.exists(combined_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done copying the relevant missing images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying the relevant missing images\n"
     ]
    }
   ],
   "source": [
    "COMBINED_ORIG_TEST_PATH = '/Users/preshita/Desktop/combined_new/test'\n",
    "COMBINED_FACE_TEST_PATH = '/Users/preshita/Desktop/combined_new_face/test'\n",
    "COMBINED_HAND_TEST_PATH = '/Users/preshita/Desktop/combined_new_hand/test'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    combined_orig_dir_path = COMBINED_ORIG_TEST_PATH + '/' + class_label\n",
    "    combined_face_dir_path = COMBINED_FACE_TEST_PATH + '/' + class_label\n",
    "    combined_hand_dir_path = COMBINED_HAND_TEST_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(combined_orig_dir_path):\n",
    "        if (not os.path.exists(combined_face_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_face_dir_path + '/' + filename)\n",
    "            \n",
    "        if (not os.path.exists(combined_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done copying the relevant missing images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying the relevant missing images\n"
     ]
    }
   ],
   "source": [
    "COMBINED_ORIG_UNSEEN_PATH = '/Users/preshita/Desktop/combined_new/unseen'\n",
    "COMBINED_FACE_UNSEEN_PATH = '/Users/preshita/Desktop/combined_new_face/unseen'\n",
    "COMBINED_HAND_UNSEEN_PATH = '/Users/preshita/Desktop/combined_new_hand/unseen'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    combined_orig_dir_path = COMBINED_ORIG_UNSEEN_PATH + '/' + class_label\n",
    "    combined_face_dir_path = COMBINED_FACE_UNSEEN_PATH + '/' + class_label\n",
    "    combined_hand_dir_path = COMBINED_HAND_UNSEEN_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(combined_orig_dir_path):\n",
    "        if (not os.path.exists(combined_face_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_face_dir_path + '/' + filename)\n",
    "            \n",
    "        if (not os.path.exists(combined_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done copying the relevant missing images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the required models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x142cbc5e0>\n",
      "<keras.engine.functional.Functional object at 0x140d61c90>\n",
      "<keras.engine.functional.Functional object at 0x140cbfbe0>\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/preshita/Desktop/')\n",
    "\n",
    "orig_model = keras.models.load_model('distracted-inception_dropout_best.hdf5')\n",
    "# Loading the model that works best on the original images\n",
    "print(orig_model)\n",
    "\n",
    "os.chdir('/Users/preshita/Desktop/CS3244-Group-8-Project-2/')\n",
    "\n",
    "# Loading the best face model\n",
    "face_model = keras.models.load_model('Final/models/best_model_face_inception.h5')\n",
    "print(face_model)\n",
    "\n",
    "# Loading the best hand model\n",
    "hand_model = keras.models.load_model('Final/models/best_model_hand_molyswu.h5')\n",
    "print(hand_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the input and output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5157 images belonging to 8 classes.\n",
      "129/129 [==============================] - 113s 856ms/step\n",
      "Found 5157 images belonging to 8 classes.\n",
      "129/129 [==============================] - 17s 121ms/step\n",
      "Found 5157 images belonging to 8 classes.\n",
      "129/129 [==============================] - 95s 730ms/step\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "orig_datagen = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "# TODO: Need to make sure that the same image is used for both models\n",
    "ORIG_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/orig/train'\n",
    "FACE_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/face/new_train'\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/hand/new_train'\n",
    "\n",
    "# Computing predictions using the best orig model\n",
    "orig_generator = orig_datagen.flow_from_directory(ORIG_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "y_orig_preds = orig_model.predict(orig_generator) # Orig data predictions\n",
    "\n",
    "# Computing predictions using the best face model\n",
    "face_test_generator = test_datagen.flow_from_directory(FACE_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "face_data_inputs = face_test_generator # Face data input\n",
    "y_face_preds = face_model.predict(face_test_generator) # Face data predictions\n",
    "\n",
    "# Computing predictions using the best hand model\n",
    "hand_test_generator = test_datagen.flow_from_directory(HANDS_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "\n",
    "hands_data_inputs = hand_test_generator # Hand data input\n",
    "y_hands_preds = hand_model.predict(hand_test_generator) # Hand data predictions\n",
    "\n",
    "# Expected data outputs\n",
    "num_of_classes = 8\n",
    "expected_output_labels = face_test_generator.classes # Expected output labels, assuming that both models are working with the same images\n",
    "data_outputs = np.zeros((expected_output_labels.size, num_of_classes))\n",
    "data_outputs[np.arange(expected_output_labels.size), expected_output_labels] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given an array contain the predictions from each classifier and the weights to be assigned to each classifier, this function computes the final weighted probability.\n",
    "'''\n",
    "def weighted_probability(num_of_classfiers, num_of_classes, networks_outputs, curr_weight_combi):\n",
    "    result = [0 for i in range(0, num_of_classes)]\n",
    "    sum = 0\n",
    "\n",
    "    for i in range(0, num_of_classfiers):\n",
    "        curr_network_output = networks_outputs[i]\n",
    "        curr_weight = curr_weight_combi[i]\n",
    "\n",
    "        for j in range(0, num_of_classes):\n",
    "            result[j] += curr_network_output[j] * curr_weight\n",
    "            sum += result[j]\n",
    "    \n",
    "    for k in range(0, num_of_classes): # Normalising to ensure that the final output is still in terms of probability\n",
    "        result[k] = result[k] / sum\n",
    "\n",
    "    return result\n",
    "\n",
    "'''\n",
    "Calculates the negative log loss.\n",
    "'''\n",
    "def fitness(y_pred, y_true): # Negative log loss function\n",
    "    return metrics.log_loss(y_true, y_pred)\n",
    "\n",
    "'''\n",
    "Randomly changes a given float number (up to 2%). \n",
    "Note: The method of mutation was not stated in the research paper. \n",
    "'''\n",
    "def mutate(weight_combi): #TODO: Need to double-check if this is okay\n",
    "    for i in range(0, len(weight_combi)):\n",
    "        weight_combi[i] = weight_combi[i] * random.uniform(0.99, 1.01)\n",
    "    \n",
    "    return weight_combi\n",
    "\n",
    "'''\n",
    "Given 2 different possible weight combination, this function produces a final weight combination by randomly extracting weight elements from either parent combinations.\n",
    "'''\n",
    "def cross_over(num_of_classifiers, parent_1, parent_2): #TODO: Need to double-check if this is okay\n",
    "    cut = random.randint(0, num_of_classifiers - 1)\n",
    "    new_weight_combi = parent_1[:cut] \n",
    "    new_weight_combi.extend(parent_2[cut:])\n",
    "\n",
    "    return new_weight_combi\n",
    "\n",
    "'''\n",
    "Produces combinations of weights that can be assigned to each of the classifiers. \n",
    "'''\n",
    "def generate_possible_weight_combis(num_of_classifiers, num_of_combis, weight_limit):\n",
    "    possible_weight_combis = []\n",
    "\n",
    "    while (num_of_combis > 0):\n",
    "        curr_weight_combi = []\n",
    "        curr_combi_len = 0\n",
    "\n",
    "        while (curr_combi_len < num_of_classifiers):\n",
    "            curr_weight = random.uniform(0, weight_limit)\n",
    "            curr_weight_combi.append(curr_weight)\n",
    "\n",
    "            curr_combi_len += 1\n",
    "        \n",
    "        possible_weight_combis.append(curr_weight_combi)\n",
    "        num_of_combis -= 1\n",
    "    \n",
    "    return possible_weight_combis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start of Genetic Algorithm to find the optimal weights for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.552235993280402, 25.618457039198482, 60.18967251597343], [1.3292377697079139, 19.300057236381672, 9.96295552336024], [10.552235993280402, 25.618457039198482, 77.20339507245673], [11.727139589191049, 52.33266797321393, 44.75062153906333], [23.265488833758553, 52.33266797321393, 44.75062153906333], [0.8423873672450455, 13.453624407598985, 77.87002418856913], [11.727139589191049, 79.31036675273418, 77.87002418856913], [88.67829478507714, 23.575405395510963, 77.20339507245673], [11.727139589191049, 79.31036675273418, 9.96295552336024], [11.727139589191049, 23.575405395510963, 77.20339507245673], [23.265488833758553, 52.33266797321393, 77.20339507245673], [0.8423873672450455, 13.453624407598985, 70.279074437823], [88.67829478507714, 19.300057236381672, 9.96295552336024], [88.67829478507714, 19.300057236381672, 9.96295552336024], [1.3292377697079139, 19.300057236381672, 9.96295552336024], [10.552235993280402, 79.31036675273418, 70.279074437823], [1.3292377697079139, 79.31036675273418, 70.279074437823], [11.727139589191049, 79.31036675273418, 70.279074437823], [10.552235993280402, 19.300057236381672, 9.96295552336024], [11.727139589191049, 79.31036675273418, 77.87002418856913]]\n",
      "[[1.3359122886788861, 78.72247930216169, 70.279074437823], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [11.727139589191049, 13.453624407598985, 70.279074437823], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [0.8423873672450455, 13.453624407598985, 70.279074437823], [11.727139589191049, 79.31036675273418, 70.279074437823], [1.3359122886788861, 78.72247930216169, 70.279074437823], [11.727139589191049, 79.31036675273418, 70.279074437823], [1.3359122886788861, 78.72247930216169, 9.96295552336024], [0.8423873672450455, 78.72247930216169, 69.93056583209305], [1.3359122886788861, 78.72247930216169, 69.93056583209305], [11.727139589191049, 13.453624407598985, 77.87002418856913], [11.727139589191049, 79.31036675273418, 70.279074437823], [10.552235993280402, 25.618457039198482, 9.96295552336024], [1.3359122886788861, 78.72247930216169, 77.87002418856913], [1.3359122886788861, 78.72247930216169, 69.93056583209305], [10.552235993280402, 79.31036675273418, 9.96295552336024], [1.3359122886788861, 78.72247930216169, 9.96295552336024], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [10.552235993280402, 25.618457039198482, 70.279074437823]]\n",
      "[[0.8423873672450455, 13.453624407598985, 78.15742939544883], [1.3359122886788861, 78.72247930216169, 70.279074437823], [1.3410616218749436, 78.23555889061629, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [1.3359122886788861, 13.453624407598985, 77.20339507245673], [1.3410616218749436, 78.23555889061629, 69.93056583209305], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [1.3359122886788861, 78.72247930216169, 77.20339507245673], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [1.3410616218749436, 78.23555889061629, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 69.93056583209305], [1.3359122886788861, 78.72247930216169, 70.279074437823], [1.3359122886788861, 13.453624407598985, 77.20339507245673], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [0.8423873672450455, 78.72247930216169, 69.93056583209305], [1.3410616218749436, 78.23555889061629, 77.20339507245673], [1.3410616218749436, 78.23555889061629, 78.15742939544883], [1.3410616218749436, 78.23555889061629, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 77.20339507245673]]\n",
      "[[0.8423873672450455, 13.453624407598985, 77.20339507245673], [0.8423873672450455, 13.453624407598985, 77.70739262048984], [0.8423873672450455, 78.23555889061629, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 69.93056583209305], [1.3410616218749436, 78.23555889061629, 77.20339507245673], [0.8423873672450455, 13.453624407598985, 77.70739262048984], [1.3410616218749436, 78.23555889061629, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 69.93056583209305], [0.848959801540317, 13.548170155122374, 78.15742939544883], [0.848959801540317, 78.23555889061629, 78.15742939544883], [1.3410616218749436, 13.453624407598985, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 77.70739262048984], [0.848959801540317, 13.548170155122374, 69.93056583209305], [0.8423873672450455, 13.453624407598985, 69.93056583209305], [0.848959801540317, 13.548170155122374, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 77.20339507245673], [1.3410616218749436, 13.453624407598985, 69.93056583209305], [0.8423873672450455, 13.453624407598985, 77.70739262048984], [0.848959801540317, 13.548170155122374, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 77.70739262048984]]\n",
      "[[0.8363221966368197, 13.495149035194002, 77.70739262048984], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8423873672450455, 13.495149035194002, 77.38224993761303], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 77.38224993761303], [0.848959801540317, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 13.495149035194002, 77.38224993761303], [1.3410616218749436, 78.23555889061629, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 77.70739262048984], [0.848959801540317, 13.495149035194002, 77.38224993761303], [0.8423873672450455, 78.23555889061629, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 77.38224993761303], [0.8423873672450455, 13.453624407598985, 77.70739262048984], [0.848959801540317, 13.548170155122374, 77.70739262048984], [0.8423873672450455, 13.453624407598985, 77.38224993761303], [0.848959801540317, 13.495149035194002, 77.38224993761303], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8423873672450455, 78.23555889061629, 78.15742939544883], [1.3410616218749436, 78.23555889061629, 78.15742939544883]]\n",
      "[[0.8363221966368197, 13.525285955317292, 77.60990336773698], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 78.23555889061629, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [1.3410616218749436, 13.453624407598985, 78.15742939544883], [0.8423873672450455, 13.495149035194002, 77.70739262048984], [1.3410616218749436, 13.495149035194002, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 78.23555889061629, 78.15742939544883], [1.3410616218749436, 78.23555889061629, 77.70739262048984], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 77.70739262048984], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [1.3410616218749436, 13.453624407598985, 77.70739262048984]]\n",
      "[[0.8425415439196515, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.447031976707548, 78.62143410076466], [0.8363221966368197, 78.23555889061629, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [0.8423873672450455, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8425415439196515, 13.447031976707548, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [0.8363221966368197, 13.495149035194002, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 78.23555889061629, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984]]\n",
      "[[0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8360806378640957, 13.453624407598985, 77.70739262048984], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 78.62143410076466], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 77.70739262048984], [0.8360806378640957, 13.453624407598985, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8363221966368197, 13.453624407598985, 78.62143410076466]]\n",
      "[[0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8360806378640957, 13.314017792129833, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.15742939544883], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8363221966368197, 13.314017792129833, 79.17950077701637], [0.8363221966368197, 13.453624407598985, 78.96232846480912], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8363221966368197, 13.475521605785014, 78.96232846480912], [0.8375662351688201, 13.314017792129833, 78.15742939544883], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8363221966368197, 13.453624407598985, 78.62143410076466], [0.8360806378640957, 13.314017792129833, 79.17950077701637]]\n",
      "[[0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8365529965206991, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8365529965206991, 13.412000482202178, 79.94020723378132], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8365529965206991, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8365529965206991, 13.412000482202178, 79.94020723378132], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.94020723378132], [0.8360806378640957, 13.314017792129833, 78.15742939544883], [0.8365529965206991, 13.412000482202178, 79.17950077701637], [0.8365529965206991, 13.412000482202178, 79.17950077701637]]\n",
      "[[0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8365529965206991, 13.412000482202178, 79.94020723378132], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.94020723378132], [0.8353908457708771, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.94020723378132], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8353908457708771, 13.211745282542134, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.94020723378132], [0.8360806378640957, 13.211745282542134, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 79.17950077701637], [0.8353908457708771, 13.314017792129833, 79.17950077701637], [0.8365529965206991, 13.412000482202178, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8365529965206991, 13.314017792129833, 79.17950077701637]]\n",
      "[[0.8305175632592212, 13.296143384805035, 79.40724005233515], [0.8353908457708771, 13.211745282542134, 79.40724005233515], [0.8305175632592212, 13.296143384805035, 80.13082482658412], [0.8305175632592212, 13.296143384805035, 79.40724005233515], [0.8305175632592212, 13.296143384805035, 79.40724005233515], [0.8353908457708771, 13.211745282542134, 79.17950077701637], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.211745282542134, 79.17950077701637], [0.8360806378640957, 13.314017792129833, 79.17950077701637], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.211745282542134, 79.17950077701637], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8305175632592212, 13.314017792129833, 79.17950077701637], [0.8353908457708771, 13.211745282542134, 79.40724005233515], [0.8360806378640957, 13.314017792129833, 80.13082482658412]]\n",
      "[[0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8305175632592212, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.211745282542134, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8305175632592212, 13.412375567013687, 79.87300787379493], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8305175632592212, 13.296143384805035, 80.13082482658412], [0.8360806378640957, 13.296143384805035, 80.13082482658412], [0.8284670259418302, 13.412375567013687, 79.87300787379493], [0.8360806378640957, 13.296143384805035, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8305175632592212, 13.296143384805035, 79.87300787379493], [0.8305175632592212, 13.211745282542134, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8305175632592212, 13.296143384805035, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8284670259418302, 13.412375567013687, 80.13082482658412]]\n",
      "[[0.8380342271964374, 13.19270879604911, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8360806378640957, 13.211745282542134, 80.13082482658412], [0.8380342271964374, 13.19270879604911, 80.64850022164245], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8380342271964374, 13.19270879604911, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.8360806378640957, 13.211745282542134, 80.13082482658412], [0.8305175632592212, 13.19270879604911, 80.64850022164245], [0.8360806378640957, 13.314017792129833, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8305175632592212, 13.211745282542134, 80.64850022164245], [0.8360806378640957, 13.211745282542134, 80.13082482658412], [0.8360806378640957, 13.314017792129833, 80.13082482658412]]\n",
      "[[0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8305175632592212, 13.19270879604911, 80.1048708965681], [0.8380342271964374, 13.19270879604911, 80.64850022164245], [0.8353908457708771, 13.261293424384586, 80.1048708965681], [0.830111236560315, 13.261293424384586, 80.13082482658412], [0.830111236560315, 13.19270879604911, 80.64850022164245], [0.8353908457708771, 13.314017792129833, 80.64850022164245], [0.8353908457708771, 13.314017792129833, 80.13082482658412], [0.830111236560315, 13.261293424384586, 80.1048708965681], [0.8305175632592212, 13.211745282542134, 80.64850022164245], [0.830111236560315, 13.211745282542134, 80.64850022164245], [0.830111236560315, 13.261293424384586, 80.64850022164245], [0.8353908457708771, 13.314017792129833, 80.64850022164245], [0.8353908457708771, 13.211745282542134, 80.13082482658412], [0.8305175632592212, 13.211745282542134, 80.64850022164245], [0.8305175632592212, 13.19270879604911, 80.13082482658412], [0.8353908457708771, 13.211745282542134, 80.1048708965681], [0.8380342271964374, 13.19270879604911, 80.64850022164245], [0.8353908457708771, 13.314017792129833, 80.64850022164245], [0.8305175632592212, 13.211745282542134, 80.64850022164245]]\n",
      "The best weight combination is: [0.830111236560315, 13.19270879604911, 80.64850022164245]\n",
      "The fitness score of this combination is: 0.05857737163726265\n"
     ]
    }
   ],
   "source": [
    "# Defining essential variables\n",
    "num_of_classifiers = 3\n",
    "num_of_classes = 8\n",
    "num_of_required_weight_combis = 20\n",
    "weight_limit = 100\n",
    "possible_weight_combis = generate_possible_weight_combis(num_of_classifiers, num_of_required_weight_combis, weight_limit)\n",
    "max_num_of_iters = 15\n",
    "\n",
    "# print(possible_weight_combis)\n",
    "\n",
    "while (max_num_of_iters > 0):\n",
    "    # Step 1: Randomly chossing 50% of the dataset to calculate the fitness scores for\n",
    "    chosen_y_true = []\n",
    "    chosen_y_orig_pred = []\n",
    "    chosen_y_face_pred = []\n",
    "    chosen_y_hand_pred = []\n",
    "\n",
    "    required_num_of_samples = len(data_outputs) // 2 # Rounding down\n",
    "\n",
    "    random_indices = []\n",
    "    while required_num_of_samples > 0:\n",
    "        curr_index = randint(0, len(data_outputs) - 1)\n",
    "\n",
    "        if (curr_index not in random_indices):\n",
    "            chosen_y_true.append(data_outputs[curr_index])\n",
    "            chosen_y_orig_pred.append(y_orig_preds[curr_index])\n",
    "            chosen_y_face_pred.append(y_face_preds[curr_index])\n",
    "            chosen_y_hand_pred.append(y_hands_preds[curr_index])\n",
    "\n",
    "            random_indices.append(curr_index)\n",
    "            required_num_of_samples -= 1\n",
    "\n",
    "    # Step 2: Calculate the average fitness scores for each of the possible weight combinations\n",
    "    fitness_and_weights = []\n",
    "\n",
    "    for weights in possible_weight_combis:\n",
    "        accumulated_fitness_score = 0\n",
    "        num_of_samples = 0\n",
    "\n",
    "        for i in range(0, len(chosen_y_true)):\n",
    "            network_outputs = [chosen_y_orig_pred[i], chosen_y_face_pred[i], chosen_y_hand_pred[i]]\n",
    "            y_pred = weighted_probability(num_of_classifiers, num_of_classes, network_outputs, weights)\n",
    "            y_true = chosen_y_true[i]\n",
    "            fitness_score = fitness(y_pred, y_true)\n",
    "            accumulated_fitness_score += fitness_score\n",
    "\n",
    "            num_of_samples += 1\n",
    "        \n",
    "        avg_fitness_score = accumulated_fitness_score / num_of_samples\n",
    "        fitness_and_weights.append((avg_fitness_score, weights))\n",
    "    \n",
    "    # print(fitness_and_weights) # For testing\n",
    "\n",
    "    # Step 3: Rank the weight combis from best to worse\n",
    "    fitness_and_weights.sort() # The combis with the lowest log loss is at the start\n",
    "    # print(fitness_and_weights) # For testing\n",
    "\n",
    "    # Step 4: Selecting parents\n",
    "    parents = []\n",
    "    curr_index = 0\n",
    "\n",
    "    # Selecting top 20% of the weight combis\n",
    "    top_20_percent = int(len(fitness_and_weights) // 5) # Rounding down\n",
    "    while (top_20_percent > 0):\n",
    "        parents.append(fitness_and_weights[curr_index][1])\n",
    "        top_20_percent -= 1\n",
    "        curr_index += 1\n",
    "\n",
    "    # Randomly choosing another 10% of the weight combinations\n",
    "    another_10_percent = int(len(fitness_and_weights) // 10)  # Rounding down\n",
    "    while(another_10_percent > 0):\n",
    "        random_score_and_parent = random.choice(fitness_and_weights[curr_index:])\n",
    "        parents.append(random_score_and_parent[1])\n",
    "        fitness_and_weights.remove(random_score_and_parent)\n",
    "\n",
    "        another_10_percent -= 1\n",
    "    \n",
    "    # print(parents) # For testing\n",
    "\n",
    "    # Step 5: Randomly mutate 5% of the selected parents\n",
    "    num_of_parents_to_mutate = max(1, int(len(parents) // 10))  # Rounding down\n",
    "    index_of_parents_to_mutate = [random.randint(0, len(parents) - 1) for i in range(0, num_of_parents_to_mutate)]\n",
    "\n",
    "    for index in index_of_parents_to_mutate:\n",
    "        parents[index] = mutate(parents[index])\n",
    "    \n",
    "    # print(parents) # For testing\n",
    "\n",
    "    # Step 6: Randomly cross over parents to produce new set of weight combinations\n",
    "    new_weight_combis = []\n",
    "    index_of_crossed_parents = []\n",
    "    num_of_curr_weights = 0\n",
    "\n",
    "    while (num_of_curr_weights < num_of_required_weight_combis):\n",
    "        chosen_parents = (random.randint(0, len(parents) - 1), random.randint(0, len(parents) - 1))\n",
    "        parent_1 = parents[chosen_parents[0]]\n",
    "        parent_2 = parents[chosen_parents[1]]\n",
    "\n",
    "        if (parent_1 != parent_2 and chosen_parents not in index_of_crossed_parents):\n",
    "            new_weight_combi = cross_over(num_of_classifiers, parent_1, parent_2)\n",
    "            new_weight_combis.append(new_weight_combi)\n",
    "            num_of_curr_weights += 1\n",
    "\n",
    "    possible_weight_combis = new_weight_combis\n",
    "    print(possible_weight_combis) # For testing\n",
    "\n",
    "    max_num_of_iters -= 1\n",
    "\n",
    "# Step 7: Select the best weights combination\n",
    "final_fitness_and_weights = []\n",
    "\n",
    "for weights in possible_weight_combis:\n",
    "    accumulated_fitness_score = 0\n",
    "    num_of_samples = 0\n",
    "\n",
    "    for i in range(0, len(chosen_y_true)):\n",
    "            network_outputs = [chosen_y_orig_pred[i], chosen_y_face_pred[i], chosen_y_hand_pred[i]]\n",
    "            y_pred = weighted_probability(num_of_classifiers, num_of_classes, network_outputs, weights)\n",
    "            y_true = chosen_y_true[i]\n",
    "            fitness_score = fitness(y_pred, y_true)\n",
    "            accumulated_fitness_score += fitness_score\n",
    "\n",
    "            num_of_samples += 1\n",
    "    \n",
    "    avg_fitness_score = accumulated_fitness_score / num_of_samples\n",
    "    final_fitness_and_weights.append((avg_fitness_score, weights))\n",
    "\n",
    "final_fitness_and_weights.sort() # The combis with the lowest log loss is at the start\n",
    "best_weights = final_fitness_and_weights[0][1]\n",
    "print(\"The best weight combination is: \" + str(best_weights))\n",
    "print(\"The fitness score of this combination is: \" + str(final_fitness_and_weights[0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the performance of the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 448 images belonging to 8 classes.\n",
      "12/12 [==============================] - 7s 581ms/step\n",
      "Found 448 images belonging to 8 classes.\n",
      "12/12 [==============================] - 1s 112ms/step\n",
      "Found 448 images belonging to 8 classes.\n",
      "12/12 [==============================] - 5s 422ms/step\n"
     ]
    }
   ],
   "source": [
    "# Measuring performance on unseen data\n",
    "ORIG_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/orig/unseen'\n",
    "FACE_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/face/unseen'\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/hand/unseen'\n",
    "\n",
    "# Computing predictions using the best orig model\n",
    "orig_generator = orig_datagen.flow_from_directory(ORIG_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "y_orig_preds = orig_model.predict(orig_generator) # Orig data predictions\n",
    "\n",
    "# Computing predictions by the best face model\n",
    "face_test_generator = test_datagen.flow_from_directory(FACE_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "face_data_inputs = face_test_generator # Face data input\n",
    "y_face_preds = face_model.predict(face_test_generator) # Face data predictions\n",
    "\n",
    "# Computing predictions by the best hand model\n",
    "hand_test_generator = test_datagen.flow_from_directory(HANDS_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "\n",
    "hands_data_inputs = hand_test_generator # Hand data input\n",
    "y_hands_preds = hand_model.predict(hand_test_generator) # Hand data predictions\n",
    "\n",
    "# Expected data outputs\n",
    "num_of_classes = 8\n",
    "expected_output_labels = face_test_generator.classes # Expected output labels, assuming that both models are working with the same images\n",
    "data_outputs = np.zeros((expected_output_labels.size, num_of_classes))\n",
    "data_outputs[np.arange(expected_output_labels.size), expected_output_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2614 images belonging to 8 classes.\n",
      "66/66 [==============================] - 54s 808ms/step\n",
      "Found 2614 images belonging to 8 classes.\n",
      "66/66 [==============================] - 8s 121ms/step\n",
      "Found 2614 images belonging to 8 classes.\n",
      "66/66 [==============================] - 48s 720ms/step\n"
     ]
    }
   ],
   "source": [
    "# Measuring performance on test data first\n",
    "ORIG_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/orig/test'\n",
    "FACE_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/face/test'\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/latest_combined_common_images/hand/test'\n",
    "\n",
    "# Computing predictions using the best orig model\n",
    "orig_generator = orig_datagen.flow_from_directory(ORIG_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "y_orig_preds = orig_model.predict(orig_generator) # Orig data predictions\n",
    "\n",
    "# Computing predictions by the best face model\n",
    "face_test_generator = test_datagen.flow_from_directory(FACE_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "face_data_inputs = face_test_generator # Face data input\n",
    "y_face_preds = face_model.predict(face_test_generator) # Face data predictions\n",
    "\n",
    "# Computing predictions by the best hand model\n",
    "hand_test_generator = test_datagen.flow_from_directory(HANDS_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "\n",
    "hands_data_inputs = hand_test_generator # Hand data input\n",
    "y_hands_preds = hand_model.predict(hand_test_generator) # Hand data predictions\n",
    "\n",
    "# Expected data outputs\n",
    "num_of_classes = 8\n",
    "expected_output_labels = face_test_generator.classes # Expected output labels, assuming that both models are working with the same images\n",
    "data_outputs = np.zeros((expected_output_labels.size, num_of_classes))\n",
    "data_outputs[np.arange(expected_output_labels.size), expected_output_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.831293\n",
      "Precision: 0.835580\n",
      "Recall: 0.831293\n",
      "F1 score: 0.831533\n"
     ]
    }
   ],
   "source": [
    "weighted_preds = []\n",
    "\n",
    "for i in range(0, len(expected_output_labels)):\n",
    "    networks_outputs = [y_orig_preds[i], y_face_preds[i], y_hands_preds[i]]\n",
    "    weighted_preds.append(weighted_probability(num_of_classifiers, num_of_classes, networks_outputs, best_weights))\n",
    "\n",
    "ensemble_ypred = np.argmax(weighted_preds, axis=1)\n",
    "\n",
    "# Printing out metrics\n",
    "accuracy = accuracy_score(expected_output_labels, ensemble_ypred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(expected_output_labels, ensemble_ypred, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(expected_output_labels, ensemble_ypred, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(expected_output_labels, ensemble_ypred, average='weighted')\n",
    "print('F1 score: %f' % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
