{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from random import randint\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done renaming\n"
     ]
    }
   ],
   "source": [
    "# Standardising the naming convention across all folders (Not part of genetic algorithm)\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/ensemble_test_hand'\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    os.chdir(HANDS_DATA_PATH)\n",
    "    curr_dir_path = HANDS_DATA_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(curr_dir_path):\n",
    "        img_num, file_ext = os.path.splitext(filename)\n",
    "        new_img_num = str(int(img_num))\n",
    "\n",
    "        if (new_img_num != img_num):\n",
    "            old_filepath = os.path.join(curr_dir_path, img_num + file_ext)\n",
    "            new_filepath = os.path.join(curr_dir_path, new_img_num + file_ext)\n",
    "            os.renames(old_filepath, new_filepath)\n",
    "\n",
    "print(\"Done renaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering common images\n"
     ]
    }
   ],
   "source": [
    "# Extracting common images (Not part of genetic algorithm)\n",
    "ORIG_FACE_DATA_PATH = '/Users/preshita/Desktop/ensemble_test_face'\n",
    "ORIG_HAND_DATA_PATH = '/Users/preshita/Desktop/ensemble_test_hand'\n",
    "ENSEMBLE_FACE_DATA_PATH = '/Users/preshita/Desktop/ensemble_test/face'\n",
    "ENSEMBLE_HAND_DATA_PATH = '/Users/preshita/Desktop/ensemble_test/hand'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    orig_face_dir_path = ORIG_FACE_DATA_PATH + '/' + class_label\n",
    "    orig_hand_dir_path = ORIG_HAND_DATA_PATH + '/' + class_label\n",
    "    ensemble_face_dir_path = ENSEMBLE_FACE_DATA_PATH + '/' + class_label\n",
    "    ensemble_hand_dir_path = ENSEMBLE_HAND_DATA_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(orig_face_dir_path):\n",
    "        if (os.path.exists(orig_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(orig_face_dir_path + '/' + filename, ensemble_face_dir_path + '/' + filename)\n",
    "            shutil.copy(orig_hand_dir_path + '/' + filename, ensemble_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done filtering common images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying the relevant missing images\n"
     ]
    }
   ],
   "source": [
    "COMBINED_ORIG_TRAIN_PATH = '/Users/preshita/Desktop/combined_new/train'\n",
    "COMBINED_FACE_TRAIN_PATH = '/Users/preshita/Desktop/combined_new_face/new_train'\n",
    "COMBINED_HAND_TRAIN_PATH = '/Users/preshita/Desktop/combined_new_hand/new_train'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    combined_orig_dir_path = COMBINED_ORIG_TRAIN_PATH + '/' + class_label\n",
    "    combined_face_dir_path = COMBINED_FACE_TRAIN_PATH + '/' + class_label\n",
    "    combined_hand_dir_path = COMBINED_HAND_TRAIN_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(combined_orig_dir_path):\n",
    "        if (not os.path.exists(combined_face_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_face_dir_path + '/' + filename)\n",
    "            \n",
    "        if (not os.path.exists(combined_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done copying the relevant missing images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying the relevant missing images\n"
     ]
    }
   ],
   "source": [
    "COMBINED_ORIG_TEST_PATH = '/Users/preshita/Desktop/combined_new/test'\n",
    "COMBINED_FACE_TEST_PATH = '/Users/preshita/Desktop/combined_new_face/test'\n",
    "COMBINED_HAND_TEST_PATH = '/Users/preshita/Desktop/combined_new_hand/test'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    combined_orig_dir_path = COMBINED_ORIG_TEST_PATH + '/' + class_label\n",
    "    combined_face_dir_path = COMBINED_FACE_TEST_PATH + '/' + class_label\n",
    "    combined_hand_dir_path = COMBINED_HAND_TEST_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(combined_orig_dir_path):\n",
    "        if (not os.path.exists(combined_face_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_face_dir_path + '/' + filename)\n",
    "            \n",
    "        if (not os.path.exists(combined_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done copying the relevant missing images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done copying the relevant missing images\n"
     ]
    }
   ],
   "source": [
    "COMBINED_ORIG_UNSEEN_PATH = '/Users/preshita/Desktop/combined_new/unseen'\n",
    "COMBINED_FACE_UNSEEN_PATH = '/Users/preshita/Desktop/combined_new_face/unseen'\n",
    "COMBINED_HAND_UNSEEN_PATH = '/Users/preshita/Desktop/combined_new_hand/unseen'\n",
    "\n",
    "class_labels = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7']\n",
    "\n",
    "for class_label in class_labels:\n",
    "    combined_orig_dir_path = COMBINED_ORIG_UNSEEN_PATH + '/' + class_label\n",
    "    combined_face_dir_path = COMBINED_FACE_UNSEEN_PATH + '/' + class_label\n",
    "    combined_hand_dir_path = COMBINED_HAND_UNSEEN_PATH + '/' + class_label\n",
    "\n",
    "    for filename in os.listdir(combined_orig_dir_path):\n",
    "        if (not os.path.exists(combined_face_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_face_dir_path + '/' + filename)\n",
    "            \n",
    "        if (not os.path.exists(combined_hand_dir_path + '/' + filename)):\n",
    "            shutil.copy(combined_orig_dir_path + '/' + filename, combined_hand_dir_path + '/' + filename)\n",
    "\n",
    "print('Done copying the relevant missing images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the required models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x135ef82e0>\n",
      "<keras.engine.functional.Functional object at 0x136f355a0>\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/preshita/Desktop/CS3244-Group-8-Project-2/')\n",
    "# Loading the best face model\n",
    "face_model = keras.models.load_model('best_model_face_inception.h5')\n",
    "print(face_model)\n",
    "\n",
    "# Loading the best face model\n",
    "hand_model = keras.models.load_model('best_model_hand_molyswu.h5')\n",
    "print(hand_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the input and output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10225 images belonging to 8 classes.\n",
      "256/256 [==============================] - 70s 269ms/step\n",
      "Found 10225 images belonging to 8 classes.\n",
      "256/256 [==============================] - 104s 399ms/step\n"
     ]
    }
   ],
   "source": [
    "# Importing data\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "# TODO: Need to make sure that the same image is used for both models\n",
    "FACE_DATA_PATH = '/Users/preshita/Desktop/combined_new_face/new_train'\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/combined_new_hand/new_train'\n",
    "\n",
    "# Computing predictions by the best face model\n",
    "face_test_generator = test_datagen.flow_from_directory(FACE_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "face_data_inputs = face_test_generator # Face data input\n",
    "y_face_preds = face_model.predict(face_test_generator) # Face data predictions\n",
    "\n",
    "# Computing predictions by the best hand model\n",
    "hand_test_generator = test_datagen.flow_from_directory(HANDS_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "\n",
    "hands_data_inputs = hand_test_generator # Hand data input\n",
    "y_hands_preds = hand_model.predict(hand_test_generator) # Hand data predictions\n",
    "\n",
    "# Expected data outputs\n",
    "num_of_classes = 8\n",
    "expected_output_labels = face_test_generator.classes # Expected output labels, assuming that both models are working with the same images\n",
    "data_outputs = np.zeros((expected_output_labels.size, num_of_classes))\n",
    "data_outputs[np.arange(expected_output_labels.size), expected_output_labels] = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given an array contain the predictions from each classifier and the weights to be assigned to each classifier, this function computes the final weighted probability.\n",
    "'''\n",
    "def weighted_probability(num_of_classfiers, num_of_classes, networks_outputs, curr_weight_combi):\n",
    "    result = [0 for i in range(0, num_of_classes)]\n",
    "    sum = 0\n",
    "\n",
    "    for i in range(0, num_of_classfiers):\n",
    "        curr_network_output = networks_outputs[i]\n",
    "        curr_weight = curr_weight_combi[i]\n",
    "\n",
    "        for j in range(0, num_of_classes):\n",
    "            result[j] += curr_network_output[j] * curr_weight\n",
    "            sum += result[j]\n",
    "    \n",
    "    for k in range(0, num_of_classes): # Normalising to ensure that the final output is still in terms of probability\n",
    "        result[k] = result[k] / sum\n",
    "\n",
    "    return result\n",
    "\n",
    "'''\n",
    "Calculates the negative log loss.\n",
    "'''\n",
    "def fitness(y_pred, y_true): # Negative log loss function\n",
    "    return metrics.log_loss(y_true, y_pred)\n",
    "\n",
    "'''\n",
    "Randomly changes a given float number (up to 2%). \n",
    "Note: The method of mutation was not stated in the research paper. \n",
    "'''\n",
    "def mutate(weight_combi): #TODO: Need to double-check if this is okay\n",
    "    for i in range(0, len(weight_combi)):\n",
    "        weight_combi[i] = weight_combi[i] * random.uniform(0.99, 1.01)\n",
    "    \n",
    "    return weight_combi\n",
    "\n",
    "'''\n",
    "Given 2 different possible weight combination, this function produces a final weight combination by randomly extracting weight elements from either parent combinations.\n",
    "'''\n",
    "def cross_over(num_of_classifiers, parent_1, parent_2): #TODO: Need to double-check if this is okay\n",
    "    cut = random.randint(0, num_of_classifiers - 1)\n",
    "    new_weight_combi = parent_1[:cut] \n",
    "    new_weight_combi.extend(parent_2[cut:])\n",
    "\n",
    "    return new_weight_combi\n",
    "\n",
    "'''\n",
    "Produces combinations of weights that can be assigned to each of the classifiers. \n",
    "'''\n",
    "def generate_possible_weight_combis(num_of_classifiers, num_of_combis, weight_limit):\n",
    "    possible_weight_combis = []\n",
    "\n",
    "    while (num_of_combis > 0):\n",
    "        curr_weight_combi = []\n",
    "        curr_combi_len = 0\n",
    "\n",
    "        while (curr_combi_len < num_of_classifiers):\n",
    "            curr_weight = random.uniform(0, weight_limit)\n",
    "            curr_weight_combi.append(curr_weight)\n",
    "\n",
    "            curr_combi_len += 1\n",
    "        \n",
    "        possible_weight_combis.append(curr_weight_combi)\n",
    "        num_of_combis -= 1\n",
    "    \n",
    "    return possible_weight_combis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start of Genetic Algorithm to find the optimal weights for each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24.32019583319557, 20.969136298038045], [24.60153591036305, 20.969136298038045], [24.60153591036305, 20.969136298038045], [18.808674877980664, 8.94421364493817], [24.32019583319557, 20.969136298038045], [24.32019583319557, 20.969136298038045], [18.808674877980664, 20.969136298038045], [18.808674877980664, 8.94421364493817], [18.808674877980664, 8.94421364493817], [24.32019583319557, 39.837186497452414]]\n",
      "[[24.60153591036305, 20.969136298038045], [24.32019583319557, 20.969136298038045], [24.60153591036305, 20.91015048637464], [24.32019583319557, 20.969136298038045], [24.60153591036305, 20.91015048637464], [24.60153591036305, 20.91015048637464], [24.802848815206186, 20.91015048637464], [24.32019583319557, 20.91015048637464], [24.802848815206186, 20.91015048637464], [24.60153591036305, 20.969136298038045]]\n",
      "[[24.32019583319557, 20.91015048637464], [24.32019583319557, 20.91015048637464], [24.581825055345007, 20.722573760991054], [24.581825055345007, 20.91015048637464], [24.581825055345007, 20.722573760991054], [24.32019583319557, 20.722573760991054], [24.581825055345007, 20.722573760991054], [24.802848815206186, 20.91015048637464], [24.581825055345007, 20.91015048637464], [24.32019583319557, 20.91015048637464]]\n",
      "[[24.581825055345007, 20.91015048637464], [24.709514809095474, 20.638447050561815], [24.581825055345007, 20.722573760991054], [24.709514809095474, 20.638447050561815], [24.581825055345007, 20.91015048637464], [24.709514809095474, 20.638447050561815], [24.709514809095474, 20.91015048637464], [24.581825055345007, 20.91015048637464], [24.581825055345007, 20.91015048637464], [24.581825055345007, 20.722573760991054]]\n",
      "[[24.927372363930047, 20.678556760513747], [24.709514809095474, 20.638447050561815], [24.581825055345007, 20.678556760513747], [24.581825055345007, 20.91015048637464], [24.709514809095474, 20.638447050561815], [24.927372363930047, 20.678556760513747], [24.581825055345007, 20.678556760513747], [24.581825055345007, 20.91015048637464], [24.709514809095474, 20.638447050561815], [24.581825055345007, 20.638447050561815]]\n",
      "[[24.865600312798186, 20.638447050561815], [24.581825055345007, 20.678556760513747], [24.927372363930047, 20.6347334283468], [24.927372363930047, 20.6347334283468], [24.865600312798186, 20.6347334283468], [24.581825055345007, 20.678556760513747], [24.865600312798186, 20.678556760513747], [24.927372363930047, 20.678556760513747], [24.581825055345007, 20.638447050561815], [24.581825055345007, 20.678556760513747]]\n",
      "[[24.812464296330912, 20.6347334283468], [24.812464296330912, 20.77625279374117], [24.812464296330912, 20.77625279374117], [24.812464296330912, 20.6347334283468], [24.812464296330912, 20.77625279374117], [24.927372363930047, 20.77625279374117], [24.927372363930047, 20.6347334283468], [24.927372363930047, 20.6347334283468], [24.812464296330912, 20.77625279374117], [24.812464296330912, 20.77625279374117]]\n",
      "[[24.812464296330912, 20.77625279374117], [24.812464296330912, 20.6347334283468], [24.96012803892518, 20.6347334283468], [24.812464296330912, 20.68361139355493], [24.96012803892518, 20.68361139355493], [24.96012803892518, 20.6347334283468], [24.96012803892518, 20.6347334283468], [24.927372363930047, 20.6347334283468], [24.812464296330912, 20.77625279374117], [24.812464296330912, 20.77625279374117]]\n",
      "[[24.96012803892518, 20.524294820528524], [24.96012803892518, 20.524294820528524], [25.19287570268719, 20.524294820528524], [25.19287570268719, 20.6347334283468], [24.96012803892518, 20.6347334283468], [24.96012803892518, 20.524294820528524], [24.96012803892518, 20.6347334283468], [25.19287570268719, 20.524294820528524], [25.19287570268719, 20.524294820528524], [24.96012803892518, 20.6347334283468]]\n",
      "[[24.90513305576891, 20.524294820528524], [24.90513305576891, 20.524294820528524], [25.19287570268719, 20.524294820528524], [25.19287570268719, 20.47730874994002], [25.19287570268719, 20.524294820528524], [25.19287570268719, 20.524294820528524], [24.90513305576891, 20.47730874994002], [24.90513305576891, 20.47730874994002], [25.19287570268719, 20.47730874994002], [24.90513305576891, 20.524294820528524]]\n",
      "The best weight combination is: [25.19287570268719, 20.47730874994002]\n",
      "The fitness score of this combination is: 0.2918008838962718\n"
     ]
    }
   ],
   "source": [
    "# Defining essential variables\n",
    "num_of_classifiers = 2\n",
    "num_of_classes = 8\n",
    "num_of_required_weight_combis = 10\n",
    "weight_limit = 50\n",
    "possible_weight_combis = generate_possible_weight_combis(num_of_classifiers, num_of_required_weight_combis, weight_limit)\n",
    "max_num_of_iters = 10\n",
    "\n",
    "# print(possible_weight_combis)\n",
    "\n",
    "while (max_num_of_iters > 0):\n",
    "    # Step 1: Randomly chossing 50% of the dataset to calculate the fitness scores for\n",
    "    chosen_y_true = []\n",
    "    chosen_y_face_pred = []\n",
    "    chosen_y_hand_pred = []\n",
    "\n",
    "    required_num_of_samples = len(data_outputs) // 2 # Rounding down\n",
    "\n",
    "    random_indices = []\n",
    "    while required_num_of_samples > 0:\n",
    "        curr_index = randint(0, len(data_outputs) - 1)\n",
    "\n",
    "        if (curr_index not in random_indices):\n",
    "            chosen_y_true.append(data_outputs[curr_index])\n",
    "            chosen_y_face_pred.append(y_face_preds[curr_index])\n",
    "            chosen_y_hand_pred.append(y_hands_preds[curr_index])\n",
    "\n",
    "            random_indices.append(curr_index)\n",
    "            required_num_of_samples -= 1\n",
    "\n",
    "    # Step 2: Calculate the average fitness scores for each of the possible weight combinations\n",
    "    fitness_and_weights = []\n",
    "\n",
    "    for weights in possible_weight_combis:\n",
    "        accumulated_fitness_score = 0\n",
    "        num_of_samples = 0\n",
    "\n",
    "        for i in range(0, len(chosen_y_true)):\n",
    "            network_outputs = [chosen_y_face_pred[i], chosen_y_hand_pred[i]]\n",
    "            y_pred = weighted_probability(num_of_classifiers, num_of_classes, network_outputs, weights)\n",
    "            y_true = chosen_y_true[i]\n",
    "            fitness_score = fitness(y_pred, y_true)\n",
    "            accumulated_fitness_score += fitness_score\n",
    "\n",
    "            num_of_samples += 1\n",
    "        \n",
    "        avg_fitness_score = accumulated_fitness_score / num_of_samples\n",
    "        fitness_and_weights.append((avg_fitness_score, weights))\n",
    "    \n",
    "    # print(fitness_and_weights) # For testing\n",
    "\n",
    "    # Step 3: Rank the weight combis from best to worse\n",
    "    fitness_and_weights.sort() # The combis with the lowest log loss is at the start\n",
    "    # print(fitness_and_weights) # For testing\n",
    "\n",
    "    # Step 4: Selecting parents\n",
    "    parents = []\n",
    "    curr_index = 0\n",
    "\n",
    "    # Selecting top 20% of the weight combis\n",
    "    top_20_percent = int(len(fitness_and_weights) // 5) # Rounding down\n",
    "    while (top_20_percent > 0):\n",
    "        parents.append(fitness_and_weights[curr_index][1])\n",
    "        top_20_percent -= 1\n",
    "        curr_index += 1\n",
    "\n",
    "    # Randomly choosing another 10% of the weight combinations\n",
    "    another_10_percent = int(len(fitness_and_weights) // 10)  # Rounding down\n",
    "    while(another_10_percent > 0):\n",
    "        random_score_and_parent = random.choice(fitness_and_weights[curr_index:])\n",
    "        parents.append(random_score_and_parent[1])\n",
    "        fitness_and_weights.remove(random_score_and_parent)\n",
    "\n",
    "        another_10_percent -= 1\n",
    "    \n",
    "    # print(parents) # For testing\n",
    "\n",
    "    # Step 5: Randomly mutate 5% of the selected parents\n",
    "    num_of_parents_to_mutate = max(1, int(len(parents) // 10))  # Rounding down\n",
    "    index_of_parents_to_mutate = [random.randint(0, len(parents) - 1) for i in range(0, num_of_parents_to_mutate)]\n",
    "\n",
    "    for index in index_of_parents_to_mutate:\n",
    "        parents[index] = mutate(parents[index])\n",
    "    \n",
    "    # print(parents) # For testing\n",
    "\n",
    "    # Step 6: Randomly cross over parents to produce new set of weight combinations\n",
    "    new_weight_combis = []\n",
    "    index_of_crossed_parents = []\n",
    "    num_of_curr_weights = 0\n",
    "\n",
    "    while (num_of_curr_weights < num_of_required_weight_combis):\n",
    "        chosen_parents = (random.randint(0, len(parents) - 1), random.randint(0, len(parents) - 1))\n",
    "        parent_1 = parents[chosen_parents[0]]\n",
    "        parent_2 = parents[chosen_parents[1]]\n",
    "\n",
    "        if (parent_1 != parent_2 and chosen_parents not in index_of_crossed_parents):\n",
    "            new_weight_combi = cross_over(num_of_classifiers, parent_1, parent_2)\n",
    "            new_weight_combis.append(new_weight_combi)\n",
    "            num_of_curr_weights += 1\n",
    "\n",
    "    possible_weight_combis = new_weight_combis\n",
    "    print(possible_weight_combis) # For testing\n",
    "\n",
    "    max_num_of_iters -= 1\n",
    "\n",
    "# Step 7: Select the best weights combination\n",
    "final_fitness_and_weights = []\n",
    "\n",
    "for weights in possible_weight_combis:\n",
    "    accumulated_fitness_score = 0\n",
    "    num_of_samples = 0\n",
    "\n",
    "    for i in range(0, len(chosen_y_true)):\n",
    "            network_outputs = [chosen_y_face_pred[i], chosen_y_hand_pred[i]]\n",
    "            y_pred = weighted_probability(num_of_classifiers, num_of_classes, network_outputs, weights)\n",
    "            y_true = chosen_y_true[i]\n",
    "            fitness_score = fitness(y_pred, y_true)\n",
    "            accumulated_fitness_score += fitness_score\n",
    "\n",
    "            num_of_samples += 1\n",
    "    \n",
    "    avg_fitness_score = accumulated_fitness_score / num_of_samples\n",
    "    final_fitness_and_weights.append((avg_fitness_score, weights))\n",
    "\n",
    "final_fitness_and_weights.sort() # The combis with the lowest log loss is at the start\n",
    "best_weights = final_fitness_and_weights[0][1]\n",
    "print(\"The best weight combination is: \" + str(best_weights))\n",
    "print(\"The fitness score of this combination is: \" + str(final_fitness_and_weights[0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the performance of the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 839 images belonging to 8 classes.\n",
      "21/21 [==============================] - 6s 281ms/step\n",
      "Found 839 images belonging to 8 classes.\n",
      "21/21 [==============================] - 6s 281ms/step\n"
     ]
    }
   ],
   "source": [
    "# Measuring performance on unseen data\n",
    "FACE_DATA_PATH = '/Users/preshita/Desktop/combined_new_face/unseen'\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/combined_new_hand/unseen'\n",
    "\n",
    "# Computing predictions by the best face model\n",
    "face_test_generator = test_datagen.flow_from_directory(FACE_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "face_data_inputs = face_test_generator # Face data input\n",
    "y_face_preds = face_model.predict(face_test_generator) # Face data predictions\n",
    "\n",
    "# Computing predictions by the best hand model\n",
    "hand_test_generator = test_datagen.flow_from_directory(HANDS_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "\n",
    "hands_data_inputs = hand_test_generator # Hand data input\n",
    "y_hands_preds = hand_model.predict(hand_test_generator) # Hand data predictions\n",
    "\n",
    "# Expected data outputs\n",
    "num_of_classes = 8\n",
    "expected_output_labels = face_test_generator.classes # Expected output labels, assuming that both models are working with the same images\n",
    "data_outputs = np.zeros((expected_output_labels.size, num_of_classes))\n",
    "data_outputs[np.arange(expected_output_labels.size), expected_output_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3414 images belonging to 8 classes.\n",
      "86/86 [==============================] - 14s 158ms/step\n",
      "Found 3414 images belonging to 8 classes.\n",
      "86/86 [==============================] - 27s 317ms/step\n"
     ]
    }
   ],
   "source": [
    "# Measuring performance on test data first\n",
    "FACE_DATA_PATH = '/Users/preshita/Desktop/combined_new_face/test'\n",
    "HANDS_DATA_PATH = '/Users/preshita/Desktop/combined_new_hand/test'\n",
    "\n",
    "# Computing predictions by the best face model\n",
    "face_test_generator = test_datagen.flow_from_directory(FACE_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "face_data_inputs = face_test_generator # Face data input\n",
    "y_face_preds = face_model.predict(face_test_generator) # Face data predictions\n",
    "\n",
    "# Computing predictions by the best hand model\n",
    "hand_test_generator = test_datagen.flow_from_directory(HANDS_DATA_PATH,\n",
    "                                                    batch_size  = 40,\n",
    "                                                    class_mode  = 'categorical', \n",
    "                                                    target_size = (100, 75), shuffle = False)\n",
    "\n",
    "hands_data_inputs = hand_test_generator # Hand data input\n",
    "y_hands_preds = hand_model.predict(hand_test_generator) # Hand data predictions\n",
    "\n",
    "# Expected data outputs\n",
    "num_of_classes = 8\n",
    "expected_output_labels = face_test_generator.classes # Expected output labels, assuming that both models are working with the same images\n",
    "data_outputs = np.zeros((expected_output_labels.size, num_of_classes))\n",
    "data_outputs[np.arange(expected_output_labels.size), expected_output_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.552724\n",
      "Precision: 0.585187\n",
      "Recall: 0.552724\n",
      "F1 score: 0.539442\n"
     ]
    }
   ],
   "source": [
    "weighted_preds = []\n",
    "\n",
    "for i in range(0, len(expected_output_labels)):\n",
    "    networks_outputs = [y_face_preds[i], y_hands_preds[i]]\n",
    "    weighted_preds.append(weighted_probability(num_of_classifiers, num_of_classes, networks_outputs, best_weights))\n",
    "\n",
    "ensemble_ypred = np.argmax(weighted_preds, axis=1)\n",
    "\n",
    "# Printing out metrics\n",
    "accuracy = accuracy_score(expected_output_labels, ensemble_ypred)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(expected_output_labels, ensemble_ypred, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(expected_output_labels, ensemble_ypred, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(expected_output_labels, ensemble_ypred, average='weighted')\n",
    "print('F1 score: %f' % f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
